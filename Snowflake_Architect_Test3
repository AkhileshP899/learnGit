Question 1
Skipped
An Architect has been asked to clone schema STAGING as it looked one week ago, Tuesday June 1st at 8:00 AM, to recover some objects.

The STAGING schema has 50 days of retention.



The Architect runs the following statement:

CREATE SCHEMA STAGING_CLONE CLONE STAGING at (timestamp => '2021-06-01 08:00:00');



The Architect receives the following error: Time travel data is not available for schema STAGING. The requested time is either beyond the allowed time travel period or before the object creation time.



The Architect then checks the schema history and sees the following:

CREATED_ON|NAME|DROPPED_ON -

2021-06-02 23:00:00 | STAGING | NULL

2021-05-01 10:00:00 | STAGING | 2021-06-02 23:00:00



How can cloning the STAGING schema be achieved?

Cloning cannot be accomplished because the STAGING schema version was not active during the proposed Time Travel time period.

Modify the statement: CREATE SCHEMA STAGING_CLONE CLONE STAGING at (timestamp => '2021-05-01 10:00:00');

Undrop the STAGING schema and then rerun the CLONE statement.

Correct answer
Rename the STAGING schema and perform an UNDROP to retrieve the previous STAGING schema version, then run the CLONE statement.

Overall explanation
For example, if you have a table with a 10-day retention period and you decrease the period to 1-day, data from days 2 to 10 will be moved into Fail-safe, leaving only the data from day 1 accessible through Time Travel.

However, the process of moving the data from Time Travel into Fail-safe is performed by a background process, so the change is not immediately visible.



Changing the retention period for your account or individual objects changes the value for all lower-level objects that do not have a retention period explicitly set.

If an object with the same name already exists, UNDROP fails. You must rename the existing object, which then enables you to restore the previous version of the object.



See this link.

See this link too.

Question 2
Skipped
What is a valid object hierarchy when building a Snowflake environment?

Organization --> Account --> Stage --> Table --> View

Account --> Schema > Table --> Stage

Correct answer
Organization --> Account --> Database --> Schema --> Stage

Account --> Database --> Schema --> Warehouse

Overall explanation
See this link.

Question 3
Skipped
After creating a database and a schema using the following commands:

CREATE OR REPLACE DATABASE MY_DB DATA_RETENTION_TIME_IN_DAYS=30;
CREATE OR REPLACE SCHEMA S1 DATA_RETENTION_TIME_IN_DAYS=50;
How long will we be able to access the data from the schema using the Time Travel functionality if we drop the database?

Correct answer
30 days.

90 days.

The default time of the account.

50 days.

Overall explanation
We are removing the database, so the schema will be removed. When a database is dropped, the data retention period for child schemas or tables, if explicitly set to be different from the retention of the database, is not honored. The child schemas or tables are retained simultaneously as the database. It would be different if we just removed the schema, where we can access this data for the next 50 days using the Time Travel functionality, as we can see in the following diagram:





See this link.

Question 4
Skipped
Which are the two limitations of the insertFiles API of Snowpipe? (Choose two.)

Correct selection
Each file path given must be <= 1024 bytes long when serialized as UTF-8.

Each file path given must be <= 512 bytes long when serialized as UTF-8.

The post can contain at most 1000 files.

Correct selection
The post can contain at most 5000 files.

The post can contain at most 10000 files.

Overall explanation
You can see more information about the insertFiles endpoint at the following link.

Question 5
Skipped
Which way to create a Snowpipe is correct if your data is hosted in AWS S3?

Correct answer
create pipe mypipe_s3
auto_ingest = true
aws_sns_topic = 'arn:aws:sns:us-west-2:000000000001:s3_mybucket' 
as
copy into snowpipe_db.public.mytable
from @snowpipe_db.public.mystage
file_format = (type = 'JSON');
create pipe mypipe_s3
auto_ingest = true
integration = "MY_INTEGRATION"
as
copy into snowpipe_db.public.mytable
from @snowpipe_db.public.mystage
file_format = (type = 'JSON');
create pipe mypipe_azure
auto_ingest = true
aws_sns_topic = 'arn:aws:sns:westus2:000000000001:azure_mybucket' 
as
copy into snowpipe_db.public.mytable
from @snowpipe_db.public.mystage
file_format = (type = 'JSON');
create pipe mypipe_s3
auto_ingest = true
aws_sns_topic = 'arn:aws:sns:westus2:000000000001:s3_mybucket' 
as
copy into snowpipe_db.public.mytable
from @snowpipe_db.public.mystage
file_format = (type = 'JSON');
Overall explanation
This is a tricky question, as the second and third statements are almost identical. In the first case, you cannot establish the param aws_sns_topic when using azure; apart from that, your data is hosted in S3, which is an AWS service.

In the third case, the region is incorrect, as there is no westus2 region on AWS. And regarding the last question, the integration param is used for Azure and Google Cloud, not AWS.

Question 6
Skipped
Which copy options are supported by the CREATE PIPE...AS COPY FROM command? (Select TWO).

Correct selection
SKIP_HEADER = <integer>

Correct selection
STRIP_OUTER_ARRAY = TRUE | FALSE

PURGE = TRUE I FALSE

ON_ERROR = ABORT_STATEMENT

FILES = ( 'file_name1' [ , 'file_name2', ... ] )

Overall explanation
All COPY INTO <table> copy options are supported except for the following:



FILES = ( 'file_name1' [ , 'file_name2', ... ] )

ON_ERROR = ABORT_STATEMENT

SIZE_LIMIT = num

PURGE = TRUE | FALSE (i.e. automatic purging while loading)

FORCE = TRUE | FALSE

RETURN_FAILED_ONLY = TRUE | FALSE

VALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS



See this link.





Question 7
Skipped
How can we add a clustering key to the existing table MYTABLE in the columns USER and CREATED_AT?

ALTER TABLE MYTABLE CLUSTER BY USER AND CLUSTER BY CREATED_AT

ALTER TABLE MYTABLE CREATE CLUSTER_KEY (USER, CREATED_AT)

ALTER TABLE MYTABLE ADD CLUSTER_KEY (USER, CREATED_AT)

Correct answer
ALTER TABLE MYTABLE CLUSTER BY (USER, CREATED_AT)

Overall explanation
The CLUSTER BY param specifies one or more columns or column expressions in the table as the clustering. By default, no clustering key is defined for the table, as clustering keys are not intended or recommended for all tables.

Question 8
Skipped
Which of the following external function rules are valid? (Choose three.)

Correct selection
External functions retum a value

Correct selection
The returned value of an external function can be a VARIANT.

External functions can appear only in certain clauses of SQL statements

External functions are not represented as database objects in Snowfiake

External functions do not impact overall performance.

Correct selection
External functions can accept parameters

Overall explanation
From the perspective of a user running a SQL statement, an external function behaves like any other UDF . External functions follow these rules:

External functions return a value.

External functions can accept parameters.

An external function can appear in any clause of a SQL statement in which other types of UDF can appear.

An external function can be part of a more complex expression

The returned value can be a compound value, such as a VARIANT that contains JSON.

External functions can be overloaded; two different functions can have the same name but different signatures (different numbers or data types of input parameters).

See this link.

Question 9
Skipped
Which of the following statements are correct about the TASK_HISTORY function? (Choose three)

Correct selection
You can query the history of task usage within a specified date range. It returns both the completed and running tasks.

The function returns a maximum of 100 rows

You can query the history of task usage within a specified date range. It only returns the completed tasks.

Correct selection
It returns the task activity within the last 7 days or the next scheduled execution within the next 8 days.

Correct selection
The function returns a maximum of 10,000 rows

Overall explanation
By default, the function returns 100 rows. However, you can specify the number of rows that you want to return (until a maximum of 10.000) with the RESULT_LIMIT parameter. It returns tasks that are SCHEDULED, EXECUTING, SUCCEEDED, FAILED, FAILED_AND_AUTO_SUSPENDED, CANCELLED, or SKIPPED, and you can also return only the ones that failed or were canceled using the ERROR_ONLY parameter.

Question 10
Skipped
What considerations should be taken into account when deciding which method to use to load data into Snowflake? (Choose three.)

The ELT method will not allow any type of data transformation on ingestion

With the ELT method, transformation may take longer to execute than other methods as they depend on the size of the data

The ETL method is typically faster than other methods, as it transforms data on ingestion

Correct selection
The ELT method is more scalable than other methods as it uses Snowflake's virtual warehouses

Correct selection
With the ETL method, users may need to reload the incoming data if there is an error in the process

Correct selection
The ELT method can separate the ingestion and transformation resources, making it the most flexible method

Overall explanation
The ETL method is typically faster than other methods, as it transforms data on ingestion -> If data transformation is performed during an ingest it is slower.

With the ETL method, users may need to reload the incoming data if there is an error in the process -> If any process fails during the transformation, as the information is not loaded in the DB, it will need to be reloaded

The ELT method is more scalable than other methods as it uses Snowflake's virtual warehouses -> As all transformation occurs within Snowflake, we will take advantage of the benefits of virtual warehouses such as scalability among others.

The ELT method can separate the ingestion and transformation resources, making it the most flexible method -> L & T are separated making each process more flexible, for example you can assign a different size warehouse for each type of process.

The ELT method will not allow any type of data transformation on ingestion -> Some kind of transformations are allowed during ingestion

With the ELT method, transformation may take longer to execute than other methods as they depend on the size of the data -> In both paradigms the transformation depends, among other things, on the data size. With the ELT method, this can be adjusted by assigning a warehouse of the appropriate size for the transformation. Transformations will take longer in the ETL method.

Question 11
Skipped
What does a successful response from the insertFiles Snowpipe endpoint mean?

Correct answer
Snowflake has recorded the list of files to add to the table.

It Retrieves a report of files submitted via the insertFile endpoint.

The files have been ingested.

Overall explanation
A 200 HTTP response from this endpoint means that Snowpipe added the files to the queue of files to ingest, but it doesn’t necessarily mean the files have been ingested.

Question 12
Skipped
What are purposes for creating a storage integration? (Choose three.)

Control access to Snowflake data using a master encryption key that is maintained in the cloud provider’s key management service.

Create private VPC endpoints that allow direct, secure connectivity between VPCs without traversing the public internet.

Manage credentials from multiple cloud providers in one single Snowflake object.

Correct selection
Support multiple external stages using one single Snowflake object.

Correct selection
Avoid supplying credentials when creating a stage or when loading or unloading data.

Correct selection
Store a generated identity and access management (IAM) entity for an external cloud provider regardless of the cloud provider that hosts the Snowflake account.

Overall explanation
A storage integration is a Snowflake object that stores a generated identity and access management (IAM) entity for your external cloud storage, along with an optional set of allowed or blocked storage locations (Amazon S3, Google Cloud Storage, or Microsoft Azure)

A single storage integration can support multiple external stages.

This option allows users to avoid supplying credentials when creating stages or when loading or unloading data.

See this link.

Question 13
Skipped
Which security feature is required to connect to Snowflake directly from an AWS VPC?

SSO

SCIM

Correct answer
PrivateLink

OAuth

Overall explanation
See this link.

Question 14
Skipped
You have a dashboard that connects to Snowflake via JDBC. The dashboard is refreshed hundreds of times per day. The data is very stable, only changing once or twice per day.

The query run by the dashboard connector user never changes. How will Snowflake manage changing and non-changing data? (Choose three.)

Correct selection
Snowflake will re-use data from the Results Cache as long as it is still the most up-to-date data available

Snowflake will spin up a warehouse each time the dashboard is refreshed

Snowflake will compile results cache data from all user results so no warehouse is needed

Correct selection
Snowflake will show the most up-to-date data each time the dashboard is refreshed

Correct selection
Snowflake will spin up a warehouse only if the underlying data has changed

Overall explanation
Until, data has not changed and query is same - Snowflake reuses the data from cache. Please note, Each time the persisted result for a query is reused, Snowflake resets the 24-hour retention period for the result, up to a maximum of 31 days from the date and time that the query was first executed. After 31 days, the result is purged and the next time the query is submitted, a new result is generated and persisted.

Question 15
Skipped
There are two databases in an account, named fin_db and hr_db which contain payroll and employee data, respectively. Accountants and Analysts in the company require different permissions on the objects in these databases to perform their jobs. Accountants need read-write access to fin_db but only require read-only access to hr_db because the database is maintained by human resources personnel.

An Architect needs to create a read-only role for certain employees working in the human resources department.

Which permission sets must be granted to this role?

USAGE on database hr_db, SELECT on all schemas in database hr_db, SELECT on all tables in database hr_db.

USAGE on database hr_db, USAGE on all schemas in database hr_db, REFERENCES on all tables in database hr_db.

MODIFY on database hr_db, USAGE on all schemas in database hr_db, USAGE on all tables in database hr_db.

Correct answer
USAGE on database hr_db, USAGE on all schemas in database hr_db, SELECT on all tables in database hr_db.

Overall explanation
This means the role has the privilege to access the database, all schemas in the database, and select data from all tables within the database. It also ensures that the role cannot modify or update any data within the hr_db database.

Question 16
Skipped
What is the best practice to follow when calling the Snowpipe loadHistoryScan endpoint?

Read the last seven days of history every hour

Correct answer
Reading the last 10 minutes of history every 8 minutes

Read the last 24 hours of history every minute

Overall explanation
This endpoint is rate limited to avoid excessive calls. To help prevent exceeding the rate limit (error code 429), Snowflake recommends relying more heavily on insertReport than loadHistoryScan. When calling loadHistoryScan endpoint, you should specify the narrowest time range that includes a set of data loads. For example, reading the last 10 minutes of history every 8 minutes would work well. Trying to read the last 24 hours of history every minute will result in 429 errors indicating a rate limit has been reached. The rate limits are designed to allow each history record to be read a handful of times.

Question 17
Skipped
At what frequency does Snowflake rotate the object keys?

Correct answer
30 Days

16 Days

60 Days

1 Year

Overall explanation
Key automatically get rotated every 30 days.

See this link.

Question 18
Skipped
Which of the following are valid context functions? (Choose three.)

CURRENT_WORKSHEET( )

Correct selection
CURRENT_CLIENT( )

CURRENT_CLOUD_INFRASTRUCTURE()

Correct selection
CURRENT_SESSION( )

Correct selection
CURRENT_REGION( )

Overall explanation
CURRENT_WORKSHEET() and CURRENT_CLOUD_INFRASTRUCTURE() are not valid context functions.

Question 19
Skipped
A healthcare company is deploying a Snowflake account that may include Personal Health Information (PHI). The company must ensure compliance with all relevant privacy standards.



Which best practice recommendations will meet data protection and compliance requirements? (Choose three.)

Correct selection
Use, at minimum, the Business Critical edition of Snowflake.

Avoid sharing data with partner organizations.

Use the Internal Tokenization feature to obfuscate sensitive data.

Correct selection
Use the External Tokenization feature to obfuscate sensitive data.

Correct selection
Create Dynamic Data Masking policies and apply them to columns that contain PHI.

Rewrite SQL queries to eliminate projections of PHI data based on current_role().

Overall explanation
Business Critical Edition offers support for PHI data (in accordance with HIPAA and HITRUST CSF regulations).

The other Snowflake features we have to leverage is Column-level Security (External tokenization and Dynamic Data Masking)

See this link.

Question 20
Skipped
Company A wants to share some data with Company B whose account is in the same region. The data that Company A would like to share is from two different databases in their account.

Which of the following is the best option that will allow Company A to share data with Company B?

Create a standard view that selects data from both tables and share the standard view.

Create a single share that includes both databases.

Correct answer
Create a secure view that selects data from both tables and share the view.

Create two shares, one for each database.

Overall explanation
Snowflake data providers can share data that resides in different databases by using secure views. A secure view can reference objects such as schemas, tables, and other views from one or more databases, as long as these databases belong to the same account.

See this link.






Question 21
Skipped
When is the parameter INTEGRATION from Snowpipe required?

Only when configuring AUTO_INGEST for Amazon S3 stages using SNS.

Correct answer
Only when configuring AUTO_INGEST for Google Cloud Storage or Microsoft Azure stages.

Only when configuring ERROR_INTEGRATION for Google Cloud Storage or Microsoft Azure stages.

Only when configuring ERROR_INTEGRATION for Amazon S3 stages using SNS.

Overall explanation
This parameter specifies the existing notification integration used to access the storage queue and is necessary for Google Cloud Storage and Azure Stages. For AWS, we will use the parameter AWS_SNS_TOPIC.

Question 22
Skipped
What Snowflake features should be leveraged when modeling using Data Vault?

Data needs to be pre-partitioned to obtain a superior data access performance

Snowflake’s ability to hash keys so that hash key joins can run faster than integer joins

Correct answer
Snowflake’s support of multi-table inserts into the data model’s Data Vault tables

Scaling up the virtual warehouses will support parallel processing of new source loads

Overall explanation
Another cool feature in Snowflake is the ability to load multiple tables at the same time using a single data source. This is called multi-table insert (MTI).

This is very useful for loading the Raw Data Vault layer in DV2 modeling, where Hubs, Links and Sats are loaded simultaneously.

Question 23
Skipped
Which are the two limitations of the insertReport API of Snowpipe? (Choose two.)

Events are retained for a maximum of 14 days.

Events are retained for a maximum of 24 hours.

Correct selection
The 10,000 most recent events are retained

Correct selection
Events are retained for a maximum of 10 minutes.

The 100,000 most recent events are retained

Overall explanation
You can see more information about the insertReport endpoint at the following link.

Question 24
Skipped
A company has a Snowflake account named ACCOUNTA in AWS us-east-1 region. The company stores its marketing data in a Snowflake database named MARKET_DB. One of the company’s business partners has an account named PARTNERB in Azure East US 2 region. For marketing purposes the company has agreed to share the database MARKET_DB with the partner account.



Which of the following steps MUST be performed for the account PARTNERB to consume data from the MARKET_DB database?

Correct answer
Create a new account (called AZABC123) in Azure East US 2 region. From account ACCOUNTA replicate the database MARKET_DB to AZABC123 and from this account set up the data sharing to the PARTNERB account.

From account ACCOUNTA create a share of database MARKET_DB, and create a new database out of this share locally in AWS us-east-1 region. Then make this database the provider and share it with the PARTNERB account.

Create a new account (called AZABC123) in Azure East US 2 region. From account ACCOUNTA create a share of database MARKET_DB, create a new database out of this share locally in AWS us-east-1 region, and replicate this new database to AZABC123 account. Then set up data sharing to the PARTNERB account.

Create a share of database MARKET_DB, and create a new database out of this share locally in AWS us-east-1 region. Then replicate this database to the partner’s account PARTNERB.

Overall explanation
To allow the account PARTNERB to consume data from the MARKET_DB database, the following steps should be performed:

Create a new Snowflake account called AZABC123 in Azure East US 2 region.

From the ACCOUNTA Snowflake account, replicate the MARKET_DB database to the AZABC123 account in Azure East US 2 region.

From the AZABC123 Snowflake account, set up data sharing to the PARTNERB account, granting appropriate privileges to access the shared objects.

Question 25
Skipped
Which statements are true about the Snowflake Spark connector's internal and external transfer modes? (Choose two.)

Correct selection
The external transfer mode uses a storage location created and managed by the user to facilitate the transfer of data between two systems

The external transfer mode uses a temporary location managed by Snowflake to facilitate the transfer of data between two systems

The internal transfer mode uses a storage location created and managed by the user to facilitate the transfer of data between two systems

Correct selection
The internal transfer mode uses a temporary location managed by Snowflake to facilitate the transfer of data between two systems

Overall explanation
The external transfer mode is recommended if the transfer is likely to take 36 hours or more, or if you are using version 2.1.x or lower of the Spark Connector, as it doesn’t support internal transfer.

Question 26
Skipped
An Architect entered the following commands in sequence:



USER1 cannot find the table.

Which of the following commands does the Architect need to run for USER1 to find the tables using the Principle of Least Privilege? (Choose two.)

GRANT OWNERSHIP ON DATABASE SANDBOX TO USER INTERN;

GRANT ALL PRIVILEGES ON DATABASE SANDBOX TO ROLE INTERN;

Correct selection
GRANT USAGE ON SCHEMA SANDBOX.PUBLIC TO ROLE INTERN;

Correct selection
GRANT USAGE ON DATABASE SANDBOX TO ROLE INTERN;

GRANT ROLE PUBLIC TO ROLE INTERN;

Overall explanation
Since each table belongs to a single schema, and the schema, in turn, belongs to a database, the table becomes the schema object, and to assign any schema object privileges, we need to first grant USAGE privilege on parent objects such as schema and database

Question 27
Skipped
Which commands can we use to return the last 1000 tasks that failed or were canceled in the last seven days?

SELECT * 
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY( 
RESULT_LIMIT => 1000 
STATUS => 'ERROR' 
))
ORDER BY SCHEDULED_TIME;
SELECT * 
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY( 
ERROR_ONLY => TRUE 
)) 
ORDER BY SCHEDULED_TIME; 
Correct answer
SELECT * 
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY( 
RESULT_LIMIT => 1000 
ERROR_ONLY => TRUE 
))
ORDER BY SCHEDULED_TIME;
SELECT * 
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY()) 
ORDER BY SCHEDULED_TIME; 
Overall explanation
You need the result_limit to return more than 100 rows in the result and the error_only parameter to return only task runs that failed or were canceled.



Question 28
Skipped
Which of the following grants are required for the role kafka_load_role_1 running the Snowflake Kafka Connector, with the intent of loading data to Snowflake? (Choose three.)

(Assume this role already exists and has usage access to the schema kafka_schema in database kafka_db, the target for data loading.)



Correct selection
grant create pipe on schema kafka_schema to role kafka_load_role_1;
grant create stream on schema kafka_schema to role kafka_load_role_1;
Correct selection
grant create table on schema kafka_schema to role kafka_load_role_1;
grant create external table on schema kafka_schema to role kafka_load_role_1;
Correct selection
grant create stage on schema kafka_schema to role kafka_load_role_1;
grant create task on schema kafka_schema to role kafka_load_role_1; 
Overall explanation
See this link.

Question 29
Skipped
A Snowflake Architect is writing a User-Defined Function (UDF) with some unqualified object names.

How will those object names be resolved during execution?

Snowflake will resolve them according to the SEARCH_PATH parameter.

Snowflake will first check the current schema, and then the PUBLIC schema of the current database.

Correct answer
Snowflake will only check the schema the UDF belongs to.

Snowflake will first check the current schema, and then the schema the previous query used.

Overall explanation
In queries, unqualified object names are resolved through a search path. The SEARCH_PATH is not used inside views or Writing User-Defined Functions (UDFs). All unqualifed objects in a view or UDF definition will be resolved in the view’s or UDF’s schema only.

See this link.

Question 30
Skipped
A company has several sites in different regions from which the company wants to ingest data.



Which of the following will enable this type of data ingestion?

The company should provision a reader account to each site and ingest the data through the reader accounts.

The company must replicate data between Snowflake accounts.

The company must have a Snowflake account in each cloud region to be able to ingest data to that account.

Correct answer
The company should use a storage integration for the external stage.

Overall explanation
The company should use a storage integration for the external stage. A storage integration allows Snowflake to connect to external cloud storage accounts such as Amazon S3, Microsoft Azure, and Google Cloud Storage. This means that the company can ingest data from multiple sites, regardless of the location, without having to create a Snowflake account in each region. Data is simply staged in the external cloud storage and then accessed by Snowflake through the storage integration.

Question 31
Skipped
A company has a table with that has corrupted data, named Data. The company wants to recover the data as it was 5 minutes ago using cloning and Time Travel.



What command will accomplish this?

CREATE TABLE Recover Data CLONE Data AT(TIME => -60*5);

Correct answer
CREATE TABLE Recover_Data CLONE Data AT(OFFSET => -60*5);

CREATE CLONE Recover_Data FROM Data AT(OFFSET => -60*5);

CREATE CLONE TABLE Recover_Data FROM Data AT(OFFSET => -60*5);

Overall explanation
This is the syntax:

CREATE [ OR REPLACE ] { DATABASE | SCHEMA | TABLE } [ IF NOT EXISTS ] <object_name> CLONE <source_object_name> [ { AT | BEFORE } ( { TIMESTAMP => <timestamp> | OFFSET => <time_difference> | STATEMENT => <id> } ) ]

See this link.

Question 32
Skipped
When will a multi-cluster warehouse start a new cluster if it’s running with the economy scaling policy?

Correct answer
Only if the system estimates there’s enough query load to keep the cluster busy for at least 6 minutes.

Only if the system estimates there’s enough query load to keep the cluster busy for at least 15 minutes.

Only if the system estimates there’s enough query load to keep the cluster busy for at least 2 minutes.

Overall explanation
You can see the differences between these scaling policies at the following table:


Question 33
Skipped
Which of the following should a Data Architect consider when configuring an API integration to create external functions in Snowflake?

Correct answer
Multiple external functions can use the same API integration object, so the same HTTPS proxy service could be used.

The role SYSADMIN has granted the global CREATE INTEGRATION privilege to other roles for a decentralized API integration.

The Snowflake default roles ACCOUNTADMIN and SYSADMIN can execute CREATE API INTEGRATION statements.

An API integration object can be used across different cloud platform accounts.

Overall explanation
Only Snowflake roles with OWNERSHIP or USAGE privileges on the API integration can use the API integration directly (e.g. by creating an external function that specifies that API integration).

An API integration object is tied to a specific cloud platform account and role within that account, but not to a specific HTTPS proxy URL. You can create more than one instance of an HTTPS proxy service in a cloud provider account, and you can use the same API integration to authenticate to multiple proxy services in that account.

Your Snowflake account can have multiple API integration objects, for example, for different cloud platform accounts.

Multiple external functions can use the same API integration object, and thus the same HTTPS proxy service.

See this link.



Question 34
Skipped
A database shared_database is created in a consumer account.



What operations can be performed on this shared database? (Select TWO).

A clone of the shared database can be created in the consumer account, with the clone being read-only.

Correct selection
An object in the shared database can be joined to objects referencing another database in the same consumer account.

The shared database created for the consumer account can be re-shared to other accounts.

Correct selection
Any number of users present in the consumer account can access the shared database and can query the data sets.

The comments on the objects in the shared database can be edited.

Overall explanation
Shared databases have the following limitations for consumers:



Shared databases are read-only. Users in a consumer account can view/query data, but cannot insert or update data, or create any objects in the database.

The following actions are not supported:

Creating a clone of a shared database or any schemas/tables in the database.

Time Travel for a shared database or any schemas/tables in the database.

Editing the comments for a shared database.

Shared databases and all the objects in the database cannot be re-shared with other accounts.

Shared databases cannot be replicated.



See this link.

Question 35
Skipped
Which privilege is required for a role to be able to resume a suspended warehouse if auto-resume is not enabled?

USAGE

MONITOR

MODIFY

Correct answer
OPERATE

Overall explanation
OPERATE: Enables changing the state of a warehouse (stop, start, suspend, resume). In addition, enables viewing current and past queries executed on a warehouse and aborting any executing queries.

See this link.

Question 36
Skipped
Consider the following COPY command which is loading data with CSV format into a Snowflake table from an internal stage through a data transformation query.




This command results in the following error:

SQL compilation error: invalid parameter 'validation_mode'

Assuming the syntax is correct, what is the cause of this error?

The VALIDATION_MODE parameter does not support COPY statements with CSV file formats.

The VALIDATION_MODE parameter supports COPY statements that load data from external stages only.

Correct answer
The VALIDATION_MODE parameter does not support COPY statements that transform data during a load.

The value return_all_errors of the option VALIDATION_MODE is causing a compilation error.

Overall explanation
VALIDATION_MODE does not support COPY statements that transform data during a load. If the parameter is specified, the COPY statement returns an error.

Question 37
Skipped
What are three of the limitations of the Search Optimization Service? (Choose three.)

Tables that are accessible by different users at the same time

Correct selection
Casts on table columns (except for fixed-point numbers cast to strings)

Correct selection
Analytical expressions

Tables with more than 100GB

Correct selection
External tables & Materialized views

Overall explanation
So, for example, the following predicate would not be supported because it uses a cast on values in the table column:

WHERE TO_DATE(varchar_column) = '2023-01-05';

You can see all the limitations of the Search Optimization Service in the following image (via docs.snowflake.com):


Question 38
Skipped
A DevOps team has a requirement for recovery of staging tables used in a complex set of data pipelines. The staging tables are all located in the same staging schema. One of the requirements is to have online recovery of data on a rolling 7-day basis.



After setting up the DATA_RETENTION_TIME_IN_DAYS at the database level, certain tables remain unrecoverable past 1 day.



What would cause this to occur? (Choose two.)

The staging schema has not been setup for MANAGED ACCESS.

The tables exceed the 1 TB limit for data recovery.

Correct selection
The DATA_RETENTION_TIME_IN_DAYS for the staging schema has been set to 1 day.

Correct selection
The staging tables are of the TRANSIENT type.

The DevOps role should be granted ALLOW_RECOVERY privilege on the staging schema.

Overall explanation
Transient tables are similar to permanent tables with the key difference that they do not have a Fail-safe period. As a result, transient tables are specifically designed for transitory data that needs to be maintained beyond each session.

Time travel 0 or 1 (default is 1)

See this link.

Changing the retention period for your account or individual objects changes the value for all lower-level objects that do not have a retention period explicitly set.

Question 39
Skipped
A table is loaded using Snowflake Connector for Kafka.



What will happen if the file cannot be loaded?

Correct answer
The Kafka Connector moves files it cannot load to the stage associated with the target table

The Kafka Connector loads the files to an error table with the columns RECORD_CONTENT and RECORD_METADATA

The Kafka Connector moves files it cannot load to the stage associated with the user loading the target table

The Kafka Connector deletes the files that cannot be loaded

Overall explanation
The Kafka connector moves files it could not load to the stage associated with the target table. The syntax for referencing a table stage is @[namespace.]%table_name.

See this link.

Question 40
Skipped
When would you usually consider to add clustering key to a table? (Choose two)

Correct selection
It is a multi-terabyte size table.

Correct selection
The performance of the query has deteriorated over a period of time.

The table has more than 20 columns.

The number of users querying the table has increased.

Overall explanation
See this link.

Question 41
Skipped
A Snowflake developer has created a masking policy with the following syntax:



create or replace masking policy mp
AS (val string) returns string ->
 CASE
 WHEN current_role() in('DEVROLE') 
  THEN val
 ELSE '*********'
END;
Which statements are correct about this policy? (Choose two.)

SECURITYADMIN role can see the plain-text value as stored in the table

Correct selection
Everyone except DEVROLE role can see the plain-text value as '*********'

The owner of the table can see the plain-text value as stored in the table

Correct selection
Anyone with DEVROLE role can see the plain-text value as stored in the table

Overall explanation
You can return whatever you want. For example, if we specify in the else:



ELSE sha2(val)



It will return a hash value using SHA2, SHA2_HEX for unauthorized users

Question 42
Skipped
Will this query cost compute credits considering that the previous query ran 5 minutes ago?



CREATE OR REPLACE TABLE MTBL AS 
SELECT * 
FROM TABLE 
(RESULT_SCAN(LAST_QUERY_ID()) 
);
No, it will not compute credits because we are re-using from the cache.

Correct answer
It will cost credits.

We cannot know because we do not know the auto-suspend time of the Warehouse.

It will cost credits if auto-suspend time of the Warehouse is less than 5 minutes.

Overall explanation
This is a tricky question. The SELECT command doesn't cost compute credits because we are re-using from the cache.

Creating the table structure doesn't cost compute credits either, BUT inserting the rows in the table requires compute power, so we will have to pay compute credits.

You can find more information about this command at the following link.

Question 43
Skipped
An Architect has created a stage for an Amazon S3 bucket Which of the following requires an SNS TOPIC to auto_refresh the object? (Choose two.)

Secure view

Correct selection
Pipe

Correct selection
External table

Search optimization

Materialized view

Overall explanation
If we want to refresh automatically an External table, we will need to use Amazon Simple Notification Services (SNS)

See this link.

Same for pipes

And this one.

Question 44
Skipped
An Architect needs to design a data unloading strategy for Snowflake, that will be used the COPY INTO <location> command.



Which configuration is valid?

• Location of files: Snowflake internal location

• File format: CSV, XML

• File encoding: UTF_8

• Encryption: 128-bit

• Location of files: Amazon S3

• File formats: CSV, JSON

• File encoding: Latin-1 (ISO-8859)

• Encryption: 128-bit

• Location of files: Azure ADLS

• File formats: JSON, XML, Avro, Parquet, ORC

• Compression: bzip2

• Encryption: User-supplied key

Correct answer
• Location of files: Google Cloud Storage

• File formats: Parquet

• File encoding. UTF-8

• Compression: gzip

Overall explanation
CSV, JSON, Parquet are allowed format files.

UTF 8 is the only encoding allowed.

See this link.

Question 45
Skipped
An Architect is making performance improvements to Snowflake processes and adjusting the sizing of virtual warehouses.

What technique does Snowflake recommend for determining which virtual warehouse size to select?

Use X-Large or above for tables larger than 1 GB

Correct answer
Experiment by running the same queries against warehouses of different sizes

Use the default size Snowflake chooses

Always start with an X-Small and increase the size if the query does not complete in 2 minutes

Overall explanation
The keys to using warehouses effectively and efficiently are:

Experiment with different types of queries and different warehouse sizes to determine the combinations that best meet your specific query needs and workload.

Don’t focus on warehouse size. Snowflake utilizes per-second billing, so you can run larger warehouses (Large, X-Large, 2X-Large, etc.) and simply suspend them when not in use.

See this link.

Question 46
Skipped
The following diagram shows the process flow for Snowpipe auto-ingest with Amazon SNS with the following three steps:

Step 1: Data files are loaded in a stage

Step 2: An Amazon S3 event notification published by SNS informs Snowpipe - by way of an SQS queue - that files are ready to load. Snowpipe copies the files into a queue.

Step 3: A Snowflake-provided virtual warehouse loads data from the queued files into the target table based on parameters defined in the specified pipe


If an AWS Administrator accidentally deletes the SQS subscription to the SNS topic in Step 2, what will happen to the pipe which references the topic to receive event messages from Amazon S3?

The pipe will continue to receive the messages as Snowflake will automatically restore the subscription to the same SNS topic and will recreate the pipe by specifying the same SNS topic name in the pipe definition.

The pipe will continue to receive the messages as Snowflake will automatically restore the subscription by creating a new SNS topic and will recreate the pipe by specifying the new SNS topic name in the pipe definition.

Correct answer
The pipe will no longer be able to receive the messages. To restore the system immediately, the user needs to manually create a new SNS topic with a different name and then recreate the pipe by specifying the new SNS topic name in the pipe definition.

The pipe will no longer be able to receive the messages and the user must wait for 24 hours from the time when the SNS topic subscription was deleted. Pipe recreation is not required as the pipe will reuse the same subscription to the existing SNS topic after 24 hours.

Overall explanation
When the SNS topic subscription or the SNS topic is deleted, any pipe that references the topic no longer receives event messages from Amazon S3.

To resolve the issue:

Wait 72 hours from the time when the SNS topic subscription was deleted.

After 72 hours, Amazon SNS clears the deleted subscription.

Recreate any pipes that reference the topic (using CREATE OR REPLACE PIPE).

All pipes that worked prior to the deletion of the SNS topic subscription should now begin to receive event messages from S3 again.

To circumvent the 72-hour delay, you can create a SNS topic with a different name. Recreate any pipes that reference the topic using the CREATE OR REPLACE PIPE command, and specify the new topic name.



See this link.

Question 47
Skipped
A company is storing large numbers of small JSON files (ranging from 1-4 bytes) that are received from IoT devices and sent to a cloud provider. In any given hour, 100,000 files are added to the cloud provider.



What is the MOST cost-effective way to bring this data into a Snowflake table?

A stream

An external table

Correct answer
A pipe

A copy command at regular intervals

Overall explanation
A pipe is a Snowflake feature that enables real-time ingestion of data from various sources, such as files, streams, or external tables. It is designed to handle high volume and high velocity data, making it ideal for use cases like the one described in the question.

In this scenario, a pipe can be set up to automatically ingest the JSON files as they are added to the cloud provider. The data can then be transformed and loaded into a Snowflake table, using a Snowflake stage as an intermediary. Pipes are highly scalable and cost-effective since they only incur charges when data is ingested, unlike other options like copying data at regular intervals or using external tables.

Question 48
Skipped
Select all the different ways that we can access to the USER_NAME field if we have a table called MYTABLE with a variant column called JSONTEXT with the following structure: (Choose two.)



{ 
"USER_NAME": "Bob", 
"USER_AGE ": 40, 
"TECHNOLOGY": "Snowflake" 
}
SELECT JSONTEXT:user_name 
FROM MYTABLE
SELECT JSONTEXT['user_name'] 
FROM MYTABLE
Correct selection
SELECT jsontext:USER_NAME 
FROM MYTABLE
Correct selection
SELECT JSONTEXT['USER_NAME'] 
FROM MYTABLE
Overall explanation
There are two ways to access a variant column in Snowflake: the dot notation and the bracket notation. Regardless of your notation, the column name is case-insensitive, but element names are case-sensitive. For that reason, it doesn’t matter if we access the JSONTEXT column in uppercase or lowercase, but we should access the USER_NAME column always with uppercase.

Question 49
Skipped
A Snowflake Architect wants to unload data from a relational table sized 5 GB using CSV. The extract needs to be as performant as possible.

What should the Architect do?

Increase the default MAX_FILE_SIZE to 5 GB and set SINGLE = true to produce a single file.

Use a regular expression in the stage specification of the COPY command to restrict parsing time.

Use Parquet as the unload file format, using Parquet's default compression feature.

Correct answer
Leave the default MAX_FILE_SIZE to 16 MB to take advantage of parallel operations.

Overall explanation
By default, COPY INTO location statements separate table data into a set of output files to take advantage of parallel operations. The maximum size for each file is set using the MAX_FILE_SIZE copy option. The default value is 16777216 (16 MB) but can be increased to accommodate larger files. The maximum file size supported is 5 GB for Amazon S3, Google Cloud Storage, or Microsoft Azure stages.

See this link.

Question 50
Skipped
A company’s client application supports multiple authentication methods, and is using Okta.



What is the best practice recommendation for the order of priority when applications authenticate to Snowflake?

Correct answer
1. OAuth (either Snowflake OAuth or External OAuth)

2. External browser

3. Okta native authentication

4. Key Pair Authentication, mostly used for service account users

5. Password

1. Okta native authentication

2. Key Pair Authentication, mostly used for production environment users

3. Password

4. OAuth (either Snowflake OAuth or External OAuth)

5. External browser, SSO

1. Password

2. Key Pair Authentication, mostly used for production environment users

3. Okta native authentication

4. OAuth (either Snowflake OAuth or External OAuth)

5. External browser, SSO

1. External browser, SSO

2. Key Pair Authentication, mostly used for development environment users

3. Okta native authentication

4. OAuth (ether Snowflake OAuth or External OAuth)

5. Password

Overall explanation
Snowflake recommends creating a spreadsheet listing all the client applications connecting to Snowflake and their authentication capabilities. If the app supports multiple authentication methods, then use the method in the below priority order.



Preference #1: OAuth (either Snowflake OAuth or External OAuth)

Preference #2: External Browser, if it's a desktop application that doesn’t support OAuth

Preference #3: Okta native authentication, if you’re using Okta, and the app supports this method while not supporting OAuth or external browser authentication yet.

Preference #4: Key Pair Authentication, mostly used for service account users. Since this requires the client application to manage private keys, complement it with your internal key management software.

Preference #5: Password, this should be the last option for applications that don’t support any of the above options. This option is commonly used for service account users connecting from 3rd party ETL apps.

Question 51
Skipped
An Architect runs the following SQL query:


How can this query be interpreted?

FILEROWS is the file format location. FILE_ROW_NUMBER is a stage.

FILEROWS is the table. FILE_ROW_NUMBER is the line number in the table.

Correct answer
FILEROWS is a stage. FILE_ROW_NUMBER is line number in file.

FILEROWS is a file. FILE_ROW_NUMBER is the file format location.

Overall explanation
See this link.

Question 52
Skipped
Which vendors do support Snowflake natively for federated authentication and SSO? (Choose two.)

Correct selection
Okta

Correct selection
Microsoft ADFS

Onelogin

Microsoft Azure Active Directory

Google G Suite

Overall explanation
Okta and Microsoft ADFS provide native Snowflake support for federated authentication and SSO. Other are not native but Snowflake supports using SAML 2.0-compliant.

Question 53
Skipped
A department from our company will run complex aggregation queries to a small subset of data from a huge table that doesn’t change much. After running these queries, we realized that the queries take a lot of time because the table is not clustered on those columns.

What is the most optimal solution that we should implement?

Create a secure view and cluster the view on those columns.

Create a view and cluster the view on those columns.

Correct answer
Create a materialized view and cluster the view on those columns.

Overall explanation
Materialized views are particularly useful in the following scenarios:


In this case, we have a combination of the first (“to a small subset of data”), second (“run complex aggregation queries”), and fourth (“that doesn’t change much”) cases in this question, so it’s a perfect example to use materialized views.

Question 54
Skipped
You have a multi-cluster warehouse running with the standard scaling policy. The maximum number of clusters is set to 8. If a lot of queries are queried, and the warehouse is constantly starting new clusters, what is the maximum time the warehouse will start all the clusters?

8 minutes.

Correct answer
160 seconds.

They all start at the same time.

80 seconds.

Overall explanation
Each successive cluster waits to start 20 seconds after the prior one has started. For example, if your warehouse is configured with ten max clusters, it can take 200+ seconds to start all 10 clusters. This doesn’t happen using the economy policy, that it will only start new clusters if the system estimates there’s enough query load to keep the cluster busy for at least 6 minutes


Question 55
Skipped
What is the difference between clustering a table and using searching optimization only talking about costs?

Search optimization involves storage and compute costs, whereas clustering only involves compute costs.

Search optimization involves storage and compute costs, whereas clustering only involves storage costs.

Correct answer
Both solutions involve storage and compute costs, so there is no difference between them talking about costs.

Search optimization involves storage and compute costs, whereas clustering doesn’t cost anything. B)

Overall explanation
See: Using the Search Optimization Service | Snowflake Documentation


Question 56
Skipped
A large join query takes around 3 hours to complete in an L warehouse. After increasing the warehouse size to XL one, the query's performance didn't improve. What can be the cause of it?

The query was still running before the new size of the warehouse was applied

The warehouse doesn't have enough power still

Correct answer
One of the column's values is significantly more than the rest of the values in the column; that's why it produces a skew in your data.

The warehouse had auto-suspended before the query finished, and it had to re-start again

Overall explanation
This is a difficult question requiring much understanding about Snowflake. Data skew primarily refers to a non-uniform distribution in a dataset. For example, in the following picture (left), we can see that most values are in the right part of the graph. In a Snowflake table, this is translated as most of the values are the same.



Because of the data skew, only one node may be processing the major portion of the data instead of distributing it along all the clusters. We can see this in the (right) part of the picture, where each square represents a warehouse cluster. So a bottleneck is produced, not improving even when we increase the warehouse size.




Question 57
Skipped
A company needs to share a Snowflake data table with data consumers who do not have Snowflake accounts.



What is the recommended way to to this?

Unload the table to a read-only cloud storage location and give consumers access to the table.

Correct answer
Create a share with the table and create a reader account with access to the share for the consumers.

Create a share with the table and provide the consumers with the public URL for the share.

Create users for the consumers that have the built-in role EXTERNAL and grant that role the select privilege on the table.

Overall explanation
As a data provider, you might want to share data with a consumer who does not already have a Snowflake account or is not ready to become a licensed Snowflake customer.

To facilitate sharing data with these consumers, you can create reader accounts. Reader accounts (formerly known as “read-only accounts”) provide a quick, easy, and cost-effective way to share data without requiring the consumer to become a Snowflake customer.

Each reader account belongs to the provider account that created it. As a provider, you use shares to share databases with reader accounts; however, a reader account can only consume data from the provider account that created it.

See this link.

Question 58
Skipped
A company is using a Snowflake account in Azure. The account has SAML SSO set up using ADFS as a SCIM identity provider. To validate Private Link connectivity, an Architect performed the following steps:

Confirmed Private Link URLs are working by logging in with a username/password account

Verified DNS resolution by running nslookups against Private Link URLs

Validated connectivity using SnowCD

Disabled public access using a network policy set to use the company’s IP address range



However, the following error message is received when using SSO to log into the company account:

IP XX.XXX.XX.XX is not allowed to access snowflake. Contact your local security administrator.



What steps should the Architect take to resolve this error and ensure that the account is accessed using only Private Link? (Choose two.)

Correct selection
Update the configuration of the Azure AD SSO to use the Private Link URLs.

Open a case with Snowflake Support to authorize the Private Link URLs’ access to the account.

Correct selection
Add the IP address in the error message to the allowed list in the network policy.

Alter the Azure security integration to use the Private Link URLs.

Generate a new SCIM access token using system$generate_scim_access_token and save it to Azure AD.

Overall explanation
Update the network policy to block public access to the account entirely. This can be done by updating the network policy to only allow traffic from the Private Link endpoints, and denying traffic from all other IP addresses.

Question 59
Skipped
Which command will use warehouse credits?

SELECT MAX(ID) 
FROM MYTABLE
Correct answer
SELECT MAX(ID) 
FROM MYTABLE 
GROUP BY ID
SELECT COUNT(*) 
FROM MYTABLE
SELECT MIN(ID) 
FROM MYTABLE 
Overall explanation
You can test it by going to the query profile of each query. The first three queries use the metadata cache, whereas the last one doesn’t do it because of the GROUP BY.

Question 60
Skipped
After running the function SYSTEM$CLUSTERING_INFORMATION in a table, it returns the following information:







What parameters indicate that the table is not well-clustered? (Choose three.)

Correct selection
Zero (0) constant micro-partitions out of 1156 total micro-partitions.

Low average of overlap depth across micro-partitions.

Correct selection
High average of overlapping micro-partitions.

Correct selection
Most micro-partitions are grouped at the lower end of the histogram, with most micro-partitions having an overlap depth between 64 and 128.

Overall explanation
A high number in the average_depth and average_overlaps indicates the table is not well-clustered; the higher the number of constant micro-partitions is, the more micro-partitions can be pruned from queries executed on the table, and in this case, is 0. Even if it doesn’t appear in the question picture, the query also returns some advice to help you identify it.

For example, after running it in my Snowflake account:


Question 61
Skipped
One query takes a lot of time, and you see in the query profiler the following information:





What might be the cause of this?

There is inefficient pruning.

The virtual warehouse is queuing queries.

The query is too large to fit in memory.

Correct answer
A "exploding join" issue might be the problem.

Overall explanation
A "exploding join" issue is produced when users join tables without providing a join condition (resulting in a "Cartesian product") or providing a condition where records from one table match multiple records from another table, producing more tuples than it consumes. As we can see, it's a case of exploding join because Snowflake has produced hundreds of thousands of records (235.8k) with an input of hundreds of records (772 & 816). You can see more information about this problem at the following link.

Question 62
Skipped
When does a stream become stale?

When it’s dropped

When it’s consumed

Correct answer
When its offset is outside of the data retention period for its source table

When it's created

Overall explanation
Stale means that the offset is outside the data retention period for the source table, and in this case, the historical data for the source table is no longer accessible. You should recreate the stream to track new changes. You should consume the stream records within a transaction during the retention period for the table. This does not apply to streams on external tables, as they don't have a data retention period.

Question 63
Skipped
What tables are we able to list using the command SHOW TABLES?

Correct answer
Tables that we have access privileges.

All the tables from a Snowflake account

All tables from a Snowflake account and the ones from the provider

Overall explanation
This command returns the table metadata and properties from the tables that you have access privileges, including dropped tables that are still within the Time Travel retention period.

Question 64
Skipped
While loading CSV data using the COPY INTO statement, a Data Engineer encounters the following error.

Number of columns in file (15) does not match that of the corresponding table (14)

Which approach should the Data Engineer take to solve this problem and load all the fields in the file into the table? (Choose two.)

Modity the file data to enclose fields with a delimiter. Leverage the FIELD_OPTIONALLY_ENCLOSED_BY option in the file format during the load process.

Modify the file data to use a different file format. Change the file format type in the file format options during the loading process.

Set the copy option FORCE to true.

Correct selection
Make necessary changes in the table DDL to support all the columns in the file.

Correct selection
Do not modify the file data. Change the ERROR_ON_COLUMN_COUNT_MISMATCH option in the file format to false during the loading process.

Overall explanation
ERROR_ON_COLUMN_COUNT_MISMATCH option is a boolean that specifies whether to generate a parsing error if the number of delimited columns (i.e. fields) in an input data file does not match the number of columns in the corresponding table.

If set to FALSE, an error is not generated and the load continues. If the file is successfully loaded:

If the input file contains records with more fields than columns in the table, the matching fields are loaded in order of occurrence in the file and the remaining fields are not loaded.

If the input file contains records with fewer fields than columns in the table, the non-matching columns in the table are loaded with NULL values. (Change the ERROR_ON_COLUMN_COUNT_MISMATCH is correct)

FORCE option specifies to load all files, regardless of whether they’ve been loaded previously and have not changed since they were loaded. (This option is not correct)



Other options will not fix the column mismatch



Question 65
Skipped
Which data transformations are supported for data loading using the COPY INTO command? (Select THREE).

Window functions

Joining to other staged data

Correct selection
Column re-ordering

Correct selection
Data type conversions

Subqueries

Correct selection
Loading a subset of the data

Overall explanation
See this link.
